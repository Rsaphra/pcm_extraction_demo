{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pydub.playback import play\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile as wav\n",
    "import statistics\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "import wave\n",
    "import contextlib\n",
    "from datetime import date\n",
    "import librosa\n",
    "from pcm_extraction import *\n",
    "from dropbox_file_handling import *\n",
    "from prev_failed_sku_handling import *\n",
    "# from airtable_df_handling import *\n",
    "import wavio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the skus of the programs that have already successfully had PCM Data extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date():\n",
    "    today = date.today()\n",
    "\n",
    "    d3 = today.strftime(\"%m_%d_%y\")\n",
    "    return d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completed_skus_from_csv(full_csv:str):\n",
    "    \"\"\"\n",
    "        Given the path of a csv of a previous sessions export, retrieves all of the skus that were completed\n",
    "        during that previous session. These will not have to be executed over again. \n",
    "        @Returns: the list of str of all skus that the program has executed over previously\n",
    "    \"\"\"\n",
    "    \n",
    "    # If there have been no previous csvs exported, then the initialized \"./pcms/initialized_0_0_0.csv\" is passed\n",
    "    # if so, return an empty list of completed skus\n",
    "    if full_csv == './pcms/initialized_0_0_0.csv':\n",
    "        return []\n",
    "    else:\n",
    "        full_pcm_df = pd.read_csv(full_csv)\n",
    "        return list(full_pcm_df['sku'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting Frequency List\n",
    "### Goal: normalize so that all are in range (-1,1), shorten down to 400 given averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_equal_parts(lst, n=400):\n",
    "    \"\"\"\n",
    "        separate a list into n (mostly) equal parts. Last index may contain less. \n",
    "    \"\"\"\n",
    "    # decrease n size for indexing purposes\n",
    "    n -= 1\n",
    "    incr = int(len(lst)/n)\n",
    "    div_list = []\n",
    "    i = 0\n",
    "    while i+incr < len(lst):\n",
    "        div_list.append(lst[i: i+incr])\n",
    "        i += incr\n",
    "    div_list.append(lst[i:-1])\n",
    "    return div_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_of_sublists(lst):\n",
    "    \"\"\"\n",
    "        given list of with sublists containing numbers, creates a new array containing only the averages\n",
    "        of each of those sublists\n",
    "        Notably bypasses the error if the length of the sublist is 0\n",
    "    \"\"\"\n",
    "    avg_list = []\n",
    "    for sublist in lst:\n",
    "        #if the sublists length is not zero, add the mean of the sublist\n",
    "        if len(sublist) != 0:\n",
    "            #with librosa the sublist will be float32 objects, must convert all to float before getting mean\n",
    "            float_list = [float(num) for num in sublist]\n",
    "            avg_list.append(statistics.mean(float_list))\n",
    "        else:\n",
    "            # append zero if there are no elements in the sublist\n",
    "            avg_list.append(0)\n",
    "    return avg_list\n",
    "\n",
    "def get_avg_of_list(lst_of_lst):\n",
    "    \"\"\"\n",
    "        given a list of lists that contain floats, get the average of each sublist and return it as a new list\n",
    "    \"\"\"\n",
    "    abs_list = [[abs(num) for num in lst] for lst in lst_of_lst]\n",
    "    new_list = get_mean_of_sublists(abs_list)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroify_edges(lst):\n",
    "    \"\"\"\n",
    "        takes in a list, then returns the same list but replacing the 1st and last elements with zeros\n",
    "    \"\"\"\n",
    "    lst[0] = 0.0\n",
    "    lst[-1] = 0.0\n",
    "    return lst\n",
    "\n",
    "def normalize_list(lst):\n",
    "    \"\"\"\n",
    "        returns new list where each element of the list has been divided by the maximum\n",
    "    \"\"\"\n",
    "    lst_max = max(lst)\n",
    "    normal_list = [round(ele/lst_max, 2) for ele in lst]\n",
    "    # for visualization purposes, make the first and last elements 0.0\n",
    "    normal_list = zeroify_edges(normal_list)\n",
    "    return normal_list\n",
    "\n",
    "\n",
    "def alternate_amplitude(amp_list):\n",
    "    \"\"\"\n",
    "        given list of amplitudes, returns the same values, but alternating between positive and negative\n",
    "        destructively edits the list\n",
    "    \"\"\"\n",
    "    for i in range(len(amp_list)):\n",
    "        # if its an odd index, replace with the negative number. \n",
    "        if i%2 == 1 and amp_list[i] != 0:\n",
    "            amp_list[i] = round(amp_list[i]*-1,2)\n",
    "    return amp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pcm_amp_list_from_data(data, size):\n",
    "    \"\"\"\n",
    "        given data, which is a list of all the amplitude values, returns a list of \n",
    "        normalized, alternating, averaged amplitudes\n",
    "    \"\"\"\n",
    "    split_data = split_into_equal_parts(data, size)\n",
    "    avg_list = get_avg_of_list(split_data)\n",
    "    norm_list = normalize_list(avg_list)\n",
    "    pcm_amp_list = alternate_amplitude(norm_list)\n",
    "    return pcm_amp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_lists(lst_1:list, lst_2:list):\n",
    "    \"\"\"\n",
    "        Averages each element in list corresponding to index\n",
    "        Intended to average together the L and R waveforms generated\n",
    "    \"\"\"\n",
    "    combined_list = [lst_1, lst_2]\n",
    "    avg_lst = [round((x+y)/2, 2) for x,y in zip(*combined_list)]\n",
    "    return avg_lst\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_string_of_list(lst):\n",
    "    \"\"\"\n",
    "        represents the list of floats as a string. Used for visualizing without line breaks\n",
    "    \"\"\"\n",
    "    str_list = [str(ele) for ele in lst]\n",
    "    joined_str = ','.join(str_list)\n",
    "    return '['+joined_str+']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_s_list_to_f_list(s:str):\n",
    "    \"\"\"\n",
    "        Takes a string that can be converted into a list, and separates them along commas, then converts each\n",
    "        Element in the list into a float and returns a list\n",
    "        \n",
    "        @Params: string with brackets and commas separating each element which can be converted to float\n",
    "        @Returns list of floats\n",
    "    \"\"\"\n",
    "    \n",
    "    #remove the brackets which would be at the beginning and end\n",
    "    s = s[1:-1]\n",
    "    s_list = s.split(',')\n",
    "    #convert all elements to float\n",
    "    f_list = [float(s) for s in s_list]\n",
    "    return f_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing the program over a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_duration(duration:float):\n",
    "    \"\"\"\n",
    "        @Params duration in seconds as a float\n",
    "        @Returns a representation of those seconds as mm:ss for formatting purposes\n",
    "    \"\"\"\n",
    "    minutes = int(duration//60)\n",
    "    seconds = int(duration%60)\n",
    "    # if there are less than 10 seconds, then we have to append the 0 beforehand. We'll convert to str in here\n",
    "    if seconds < 10:\n",
    "        seconds = '0'+str(seconds)\n",
    "    else:\n",
    "        seconds = str(seconds)\n",
    "    return str(minutes)+':'+seconds\n",
    "\n",
    "def get_duration_from_wav_file(sku, wav_file:str):\n",
    "    \"\"\"\n",
    "        given a string of the location of a wav file, returns the duration of that wav file in seconds (int)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with contextlib.closing(wave.open(wav_file,'r')) as f:\n",
    "            frames = f.getnframes()\n",
    "            rate = f.getframerate()\n",
    "            duration = frames / float(rate)\n",
    "            return int(duration)\n",
    "    except:\n",
    "        failed_skus.append(sku)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_sku_from_wav_file(wav_file:str):\n",
    "    \"\"\"\n",
    "        given an abosolute path, will isolate just the SKU that is at the front of the filename\n",
    "    \"\"\"\n",
    "    filename = wav_file.split('/')[-1]\n",
    "    #isolate from .wav by splitting at the '.' and taking the first part\n",
    "    title = filename.split('.')[0]\n",
    "    # get only the SKU, as in nothing after the first '_' if there is one\n",
    "    sku = title.split('_')[0]\n",
    "    return sku\n",
    "\n",
    "def get_pcm_data_from_wav_file_w_librosa(sku, wav_file:str, sample_rate=12800):\n",
    "    \"\"\"\n",
    "        Identical function to \"get_pcm_from_wav_file\", but uses the librosa library instead, also changes sample\n",
    "        rate to decrease the total number of values in the array. \n",
    "        \n",
    "        @params: wav_file as a string of the absolute path of a wav file, sku of the file as a str, sample rate\n",
    "        for getting wav amplitudes, defaults to 200 (very low resolution)\n",
    "        @returns: list of floats rounded to 2 decimal places\n",
    "    \"\"\"\n",
    "#     try:\n",
    "    #wav amps is the list of amplitudes at \n",
    "    wav_amps, sr = librosa.load(wav_file, mono=True, sr=sample_rate)\n",
    "    # 400 is the length of the array that is returned\n",
    "    pcm_data = get_pcm_amp_list_from_data(wav_amps, 400)\n",
    "    return pcm_data\n",
    "#     except:\n",
    "    print(sku+': Failed')\n",
    "    failed_skus.append(sku)\n",
    "    return []\n",
    "    \n",
    "    \n",
    "def get_pcm_data_from_wav_file_test(sku, wav_file:str, stereo=False):\n",
    "    #data here has both the R and L channels\n",
    "    data = wavio.read(wav_file).data\n",
    "    #get the waveform data from the right channel\n",
    "    dataR = [datum[0] for datum in data]\n",
    "    pcm_data_R = get_pcm_amp_list_from_data(dataR, 400)\n",
    "\n",
    "    # if we want the stereo version, then we'll also take in the left track and average the two\n",
    "    if stereo:\n",
    "        dataL = [datum[1] for datum in data]\n",
    "        pcm_data_L = get_pcm_amp_list_from_data(dataL, 400)\n",
    "        pcm_data_full = average_lists(pcm_data_R, pcm_data_L)\n",
    "        return pcm_data_full\n",
    "    else:\n",
    "        return pcm_data_R\n",
    "\n",
    "def get_pcm_data_from_wav_file(sku, wav_file:str, stereo=True):\n",
    "    \"\"\"\n",
    "        given an absolute path of a wav file, returns a list of 400 numbers between 0 and 1 that represent the\n",
    "        waveform of the file. Normally only returns the waveform data for the Right channel, \n",
    "        but if stereo is set to true, it will average both left and right. \n",
    "        \n",
    "        @params: wav_file as a string of the absolute path of a wav file, stereo, true if average of channel is sought\n",
    "        @returns: list of floats rounded to 2 decimal places\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        wavio_file = wavio.read(wav_file)\n",
    "        data = wavio_file.data\n",
    "        dataR = [datum[0] for datum in data]\n",
    "        pcm_data_R = get_pcm_amp_list_from_data(dataR, 400)\n",
    "\n",
    "        # if we want the stereo version, then we'll also take in the left track and average the two\n",
    "        if stereo:\n",
    "            dataL = [datum[1] for datum in data]\n",
    "            pcm_data_L = get_pcm_amp_list_from_data(dataL, 400)\n",
    "            pcm_data_full = average_lists(pcm_data_R, pcm_data_L)\n",
    "            return pcm_data_full\n",
    "        else:\n",
    "            return pcm_data_R\n",
    "    except:\n",
    "        try:\n",
    "            #if there is an error with the wav.read, next try the librosa.read function\n",
    "            print('File Extracted w Librosa')\n",
    "            return get_pcm_data_from_wav_file_w_librosa(sku, wav_file)\n",
    "        except:\n",
    "            #if that also fails, return an empty array and append the value to the failed skus\n",
    "            failed_skus.append(sku)\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_representation_of_row(sku, pcm, duration):\n",
    "    \"\"\"\n",
    "        Given the sku, pcm and duration (ORDER MATTERS), will return a dictionary with the keys being the\n",
    "        column names and the values being the input. \n",
    "        So that dict can be appended to dataframe\n",
    "    \"\"\"\n",
    "    new_row = {}\n",
    "    new_row['sku'] = sku\n",
    "    new_row['meta:wlk_pcm_data'] = pcm\n",
    "    new_row['meta:wlk_track_length'] = duration\n",
    "    return new_row\n",
    "\n",
    "def create_series_from_data(cols:list, data:list):\n",
    "    \"\"\"\n",
    "        Given a list of cols and a list of data, which correspond to those cols based on index, \n",
    "        returns a pd.Series object that can be easily added to a dataframe\n",
    "    \"\"\"  \n",
    "    row = pd.Series()\n",
    "    #make sure that the two lists are of equal length, otherwise they cannot correspond\n",
    "    if len(cols) != len(data):\n",
    "        #if not, return the empty series\n",
    "        return row\n",
    "    for i in range(len(cols)):\n",
    "        col = cols[i]\n",
    "        col_datum = data[i]\n",
    "        row[col] = col_datum\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_w_pcm_data(wav_files:list, cols, index_col, completed_skus:list):\n",
    "    \"\"\"\n",
    "        given a list where each element is the absolute path of a wav file file, get the pcm data for that wav\n",
    "        file and save it to a dataframe. \n",
    "        Only executes over the skus that have not been completed during a previous run.\n",
    "        @Params: completed_skus is a list that contains all the skus that the program has already completed. \n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    for wav_file in wav_files:\n",
    "        sku = get_sku_from_wav_file(wav_file)\n",
    "        # only execute the rest of this if the sku hasn't already been completed before\n",
    "        if sku not in completed_skus:\n",
    "            pcm_data = get_pcm_data_from_wav_file_w_librosa(sku, wav_file)\n",
    "            duration = get_duration_from_wav_file(sku, wav_file)\n",
    "            # bundle all data together in a list so that it can be passed to create a series\n",
    "            data = [sku, pcm_data, duration]\n",
    "            new_row = create_series_from_data(cols, data)\n",
    "            df = df.append(new_row, ignore_index=True)\n",
    "            #add to the global variable of current completed skus\n",
    "            curr_completed_skus.append(sku)\n",
    "            \n",
    "        \n",
    "    #set the index col before returning \n",
    "    df.set_index(index_col, inplace=True)\n",
    "    return df\n",
    "\n",
    "def export_df_as_csv(df, title='pcm_data_'):\n",
    "    \"\"\"\n",
    "        given dataframe, it exports it as a csv to the current directory with current date\n",
    "    \"\"\"\n",
    "    subdirectory = './pcms/'\n",
    "    file_designation = title\n",
    "    date = get_date()\n",
    "    filename = subdirectory+file_designation+date+'.csv'\n",
    "    df.to_csv(filename)\n",
    "    \n",
    "def export_completed_skus_as_csv(curr_completed_skus, old_completed_skus):\n",
    "    \"\"\"\n",
    "        Takes in the old list of completed skus and the new list of completed skus and exports \n",
    "        a csv in the current directory with the name \"completed_skus.csv\". Will overwrite the old version\n",
    "    \"\"\"\n",
    "    all_completed_skus = curr_completed_skus+old_completed_skus\n",
    "    # we will first convert to a pd dataframe before exporting as a csv\n",
    "    completed_dict = {'sku':all_completed_skus}\n",
    "    complete_skus_df = pd.DataFrame.from_dict(completed_dict)\n",
    "    filename = 'completed_skus.csv'\n",
    "    complete_skus_df.to_csv(filename)\n",
    "\n",
    "\n",
    "def get_pcm_data(path, sku_range=0, after=True):\n",
    "    \"\"\"\n",
    "        Main method\n",
    "        Takes in the path of the directory and will execute through all wav files in that directory and export a\n",
    "        CSV containing each tracks pcm data and track length with its corresponding SKU\n",
    "    \n",
    "    \"\"\"\n",
    "    all_wav_files = get_filenames_in_range(path, sku_range, after)\n",
    "    cols = ['sku', 'meta:wlk_pcm_data', 'meta:wlk_track_length']\n",
    "    \n",
    "    # get the most recent export in the pcms folder so that we only have to execute program for new files\n",
    "    prev_exports_dir = './pcms/'\n",
    "    most_recent_export_path = prev_exports_dir+get_most_recent_file_in_dir(prev_exports_dir)\n",
    "    completed_skus = get_completed_skus_from_csv(most_recent_export_path)\n",
    "    \n",
    "    # if there has never been a previous export, then skip reading the previous export as a df\n",
    "    if not completed_skus:\n",
    "        full_pcm_df = create_df_w_pcm_data(all_wav_files, cols, 'sku', completed_skus)\n",
    "    else:\n",
    "        #create new df with all the wav files that were executed over last time\n",
    "        df_prev = pd.read_csv(most_recent_export_path)\n",
    "        #reset the index to sku on the previous one for concatenating later\n",
    "        df_prev.set_index('sku', inplace=True)  \n",
    "        df_new = create_df_w_pcm_data(all_wav_files, cols, 'sku', completed_skus)\n",
    "        full_pcm_df = pd.concat([df_new, df_prev])\n",
    "    #sort by the skus\n",
    "    full_pcm_df.sort_values('sku', inplace=True)\n",
    "    export_df_as_csv(full_pcm_df)\n",
    "    # if there were previous executions to pull from, return the df of the new ones added\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_from_dir_w_sku(directory, sku):\n",
    "    \"\"\"\n",
    "        @Params the sku of the desired file\n",
    "        @Returns the full path of the file including filename corresponding to that sku\n",
    "    \"\"\"\n",
    "    all_files = get_all_filenames(directory)\n",
    "    for file in all_files:\n",
    "        if sku in file:\n",
    "            return file\n",
    "\n",
    "def get_all_filenames(directory):\n",
    "    all_wavs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".wav\"): \n",
    "             # print(os.path.join(directory, filename))\n",
    "            all_wavs.append(directory+filename)\n",
    "    return all_wavs\n",
    "  \n",
    "def get_sku_num_from_filename(filename:str):\n",
    "    \"\"\"\n",
    "        Retrieves the sku number as an integer based on the filename in the standard format\n",
    "        \n",
    "        @Params: string of a filename where everything before the first '-' is an sku, as an int\n",
    "    \"\"\"\n",
    "    sku_num = filename.split('-')[1]\n",
    "    return int(sku_num)\n",
    "\n",
    "def get_sku_from_filepath(filepath:str):\n",
    "    \"\"\"\n",
    "        Retrieves the sku from the full filepath of a wav.\n",
    "        @Returns: the sku in front of the filename\n",
    "    \"\"\"\n",
    "    filename = filepath.split('/')[-1]\n",
    "    sku_num = get_sku_num_from_filename(filename)\n",
    "    return 'TR-'+str(sku_num)\n",
    "\n",
    "def get_filenames_in_range(directory, sku_max, after=True):\n",
    "    \"\"\"\n",
    "        \n",
    "    \"\"\"\n",
    "    all_wavs = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            #if after is true, then only get files with sku greater than the sku specified\n",
    "            if after:\n",
    "                if get_sku_num_from_filename(filename) >= sku_max:\n",
    "                    all_wavs.append(directory+filename)\n",
    "            else:\n",
    "                if get_sku_num_from_filename(filename) <= sku_max:\n",
    "                    all_wavs.append(directory+filename)\n",
    "    return all_wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_file(filename:str):\n",
    "    \"\"\"\n",
    "        Given a file as a string, returns just the date that is appended at the end, before the file classifier\n",
    "        @Returns as a list of the 2 numbers with [[0]month,[1]day,[2]year]\n",
    "    \"\"\"\n",
    "    #remove the .filetype at the end\n",
    "    file = filename.split('.')[-2]\n",
    "    #Get only the last 3 numbers which are the Month, Day, Year\n",
    "    date = file.split('_')[-3:]\n",
    "    date_as_ints = [int(day) for day in date]\n",
    "    return date_as_ints\n",
    "\n",
    "def compare_dates(date1:list, date2:list):\n",
    "    \"\"\"\n",
    "        Date1 and Date2 are lists containing elements where [0] is month, [1] is day, [2] is year\n",
    "        Returns the date that is most recent. \n",
    "    \"\"\"\n",
    "    if date1[2] > date2[2]:\n",
    "        return date1\n",
    "    elif date1[2] < date2[2]:\n",
    "        return date2\n",
    "    #if the years are the same\n",
    "    else:\n",
    "        #compare the months\n",
    "        if date1[0] > date2[0]:\n",
    "            return date1\n",
    "        elif date1[0] < date2[0]:\n",
    "            return date2\n",
    "        else:\n",
    "            #compare days\n",
    "            if date1[1] > date2[1]:\n",
    "                return date1\n",
    "            #if they are equal, then it doesn't matter which one is returned\n",
    "            else:\n",
    "                return date2            \n",
    "\n",
    "def compare_file_by_date(file1:str, file2:str):\n",
    "    \"\"\"\n",
    "        Given two filenames, returns the file that was published at a later date\n",
    "    \"\"\"\n",
    "    file1_date = get_date_from_file(file1)\n",
    "    file2_date = get_date_from_file(file2)\n",
    "    later_date = compare_dates(file1_date, file2_date)\n",
    "    if later_date == file1_date:\n",
    "        return file1\n",
    "    else:\n",
    "        return file2\n",
    "    \n",
    "\n",
    "def get_most_recent_file_in_dir(directory):\n",
    "    \"\"\"\n",
    "        Given a directory with a bunch of files that have the date appended to the end of them (preceding\n",
    "        the file type), returns the file in that directory with the latest date\n",
    "    \"\"\"\n",
    "    # the initialized file has to be overwritten immediately. Must be in the format, but must be equivalent to 0\n",
    "    initialized_date = 'initialized_0_0_0.csv'\n",
    "    most_recent = initialized_date\n",
    "    for filename in os.listdir(directory):\n",
    "        # There is the .ds store file in this folder, it must be ignored. \n",
    "        if filename != '.DS_Store':\n",
    "            most_recent = compare_file_by_date(filename, most_recent)\n",
    "    return most_recent\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing previously failed skus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_previously_failed_skus(prev_df):\n",
    "    \"\"\"\n",
    "        Goes through the previously completed export of a dataframe and returns all skus where the\n",
    "        pcm value was an empty list\n",
    "        @Returns: list of previously failed skus as strings\n",
    "    \"\"\"\n",
    "    #initialize an empty list of the previously failed skus\n",
    "    missed_skus = []\n",
    "    for index, row in prev_df.iterrows():\n",
    "        if row['meta:wlk_pcm_data']=='[]':\n",
    "            missed_skus.append(row['sku'])\n",
    "    return missed_skus\n",
    "\n",
    "def get_filepaths_of_prev_failed_skus(prev_failures:list):\n",
    "    \"\"\"\n",
    "        Given a list of previously failed skus (strings), returns a list of the full filepath of \n",
    "        each of their corresponding files\n",
    "    \"\"\"\n",
    "    failed_file_paths = []\n",
    "    for failed_sku in prev_failures:\n",
    "        failed_file_paths.append(get_filename_from_dir_w_sku(path, failed_sku))\n",
    "    return failed_file_paths\n",
    "\n",
    "def get_dict_of_retried_skus(prev_failed_skus:list):\n",
    "    \"\"\"\n",
    "        Given the skus of the skus that failed in the previous export, retries to extract pcm data with the\n",
    "        librosa method instead. \n",
    "        @Returns Dicionary where key is the sku of the file, and value is the list of PCM data\n",
    "    \"\"\"\n",
    "    path = '/Users/dsaphra/Dropbox/_TuneReel/Library/WAV Retitled/'\n",
    "    failed_filepaths = get_filepaths_of_prev_failed_skus(prev_failed_skus)\n",
    "    retried_file_pcms = {}\n",
    "    for failed_file_path in failed_file_paths:\n",
    "        file_sku = get_sku_from_filepath(failed_file_path)\n",
    "        print(failed_file_path)\n",
    "        file_pcm = get_pcm_data_from_wav_file_w_librosa(file_sku, failed_file_path)\n",
    "        retried_file_pcms[file_sku] = file_pcm\n",
    "    return retried_file_pcms\n",
    " \n",
    "def add_retried_file_pcms_to_prev_dataframe(prev_df, new_pcm_dict):\n",
    "    \"\"\"\n",
    "        Easiest way to reincorporate the successful pcm extractions is to fill in the meta:wlk_pcm_data column in \n",
    "        the previous dataframe with the newly added pcm data\n",
    "        @Returns: new dataframe, identical but with updated pcm cells for previously failed skus\n",
    "    \"\"\"\n",
    "    new_pcm_skus = list(new_pcm_dict.keys())\n",
    "    df = prev_df.set_index('sku')\n",
    "    display(df.at['TR-3466-A','meta:wlk_pcm_data'])\n",
    "    for sku in new_pcm_skus:\n",
    "        # All the skus added at this point (might be temporary) had the suffix -A, so it needs to be added\n",
    "        df_sku = sku+'-A'\n",
    "        df.at[df_sku, 'meta:wlk_pcm_data'] = new_pcm_dict[sku]\n",
    "    return df\n",
    "        \n",
    "    \n",
    "def retry_previous_failures(prev_df_csv:str):\n",
    "    \"\"\"\n",
    "        Go through the most previous export, and if there are any rows that ended up with empty values under\n",
    "        pcm data, retry to extract the PCM data from those files\n",
    "        @Params: Previous df as a csv file\n",
    "    \"\"\"\n",
    "    prev_df = pd.read_csv(prev_df_csv)\n",
    "    prev_failed_skus = get_previously_failed_skus(prev_df)\n",
    "    retried_pcm_dict = get_dict_of_retried_skus(prev_failed_skus)\n",
    "    updated_df = add_retried_file_pcms_to_prev_dataframe(prev_df, retried_pcm_dict)\n",
    "    export_df_as_csv(updated_df)\n",
    "    return updated_df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    path = '/Users/dsaphra/Dropbox/_TuneReel/Library/WAV Retitled/'\n",
    "    new_df = get_pcm_data(path, 0)\n",
    "    display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/dsaphra/Dropbox/_TuneReel/Library/WAV Retitled/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b55ae78267a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#curr completed skus is the skus that this session of the program have been completed. Will be exported at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcurr_completed_skus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-b125d674f584>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/dsaphra/Dropbox/_TuneReel/Library/WAV Retitled/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnew_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pcm_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-d8a00f9c9a6e>\u001b[0m in \u001b[0;36mget_pcm_data\u001b[0;34m(path, sku_range, after)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mall_wav_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filenames_in_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msku_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'sku'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'meta:wlk_pcm_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'meta:wlk_track_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-0801489b45b3>\u001b[0m in \u001b[0;36mget_filenames_in_range\u001b[0;34m(directory, sku_max, after)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[1;32m     41\u001b[0m     \u001b[0mall_wavs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m#if after is true, then only get files with sku greater than the sku specified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/dsaphra/Dropbox/_TuneReel/Library/WAV Retitled/'"
     ]
    }
   ],
   "source": [
    "failed_skus = []\n",
    "#curr completed skus is the skus that this session of the program have been completed. Will be exported at the end\n",
    "curr_completed_skus = []\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0,-0.54,0.26,-0.18,0.36,-0.37,0.13,-0.06,0.52,-0.29,0.2,-0.27,0.44,-0.2,0.1,-0.38,0.39,-0.23,0.21,-0.47,0.28,-0.14,0.25,-0.4,0.27,-0.23,0.53,-0.33,0.21,-0.13,0.51,-0.36,0.31,-0.43,0.58,-0.39,0.25,-0.6,0.47,-0.4,0.39,-0.62,0.48,-0.34,0.47,-0.65,0.49,-0.37,0.62,-0.41,0.41,-0.42,0.57,-0.52,0.5,-0.63,0.56,-0.66,0.57,-0.84,0.93,-0.67,0.75,-0.48,0.54,-0.37,0.49,-0.54,0.38,-0.3,0.47,-0.42,0.33,-0.39,0.44,-0.41,0.31,-0.39,0.47,-0.32,0.33,-0.51,0.54,-0.32,0.34,-0.46,0.19,-0.1,0.37,-0.42,0.39,-0.32,0.46,-0.44,0.36,-0.42,0.42,-0.43,0.36,-0.44,0.46,-0.35,0.36,-0.47,0.44,-0.4,0.36,-0.47,0.41,-0.37,0.39,-0.38,0.29,-0.3,0.38,-0.41,0.59,-0.75,0.83,-0.5,0.73,-0.61,0.45,-0.62,0.75,-0.59,0.92,-0.71,0.77,-0.73,0.71,-0.94,0.81,-0.67,0.66,-0.71,0.53,-0.57,0.51,-0.92,0.83,-0.77,0.71,-0.72,0.78,-0.84,0.81,-0.58,0.4,-0.58,0.4,-0.4,0.43,-0.62,0.68,-0.54,0.8,-0.54,0.52,-0.7,0.68,-0.7,0.4,-0.67,0.43,-0.37,0.49,-0.51,0.69,-0.63,0.53,-0.45,0.42,-0.52,0.73,-0.81,0.92,-0.47,0.95,-0.72,0.74,-0.47,0.95,-0.84,0.36,-0.74,0.69,-0.81,0.84,-1.0,0.77,-0.53,0.71,-0.95,0.88,-0.29,0.89,-0.85,0.62,-0.68,0.78,-0.64,0.64,-0.85,0.73,-0.81,0.65,-0.93,0.9,-0.51,0.77,-0.82,0.87,-0.5,0.92,-0.68,0.53,-0.49,0.75,-0.99,0.51,-0.91,0.77,-0.68,0.62,-0.83,0.88,-0.54,0.57,-0.45,0.43,-0.47,0.86,-0.68,0.59,-0.58,0.33,-0.34,0.31,-0.76,0.57,-0.43,0.49,-0.26,0.21,-0.19,0.47,-0.53,0.49,-0.45,0.31,-0.25,0.25,-0.41,0.71,-0.48,0.48,-0.39,0.26,-0.27,0.26,-0.65,0.45,-0.51,0.47,-0.34,0.3,-0.21,0.59,-0.53,0.43,-0.53,0.3,-0.29,0.25,-0.49,0.62,-0.52,0.47,-0.4,0.3,-0.43,0.44,-0.55,0.5,-0.54,0.45,-0.32,0.3,-0.28,0.65,-0.5,0.46,-0.48,0.33,-0.32,0.34,-0.64,0.58,-0.73,0.54,-0.51,0.48,-0.54,0.6,-0.7,0.85,-0.56,0.54,-0.44,0.48,-0.57,0.73,-0.67,0.54,-0.57,0.49,-0.5,0.46,-0.69,0.56,-0.63,0.58,-0.66,0.83,-0.56,0.73,-0.6,0.69,-0.5,0.51,-0.57,0.63,-0.69,0.71,-0.75,0.62,-0.59,0.59,-0.65,0.64,-0.87,0.74,-0.61,0.66,-0.61,0.62,-0.62,0.8,-0.78,0.68,-0.72,0.83,-0.81,0.73,-0.77,0.68,-0.71,0.61,-0.67,0.71,-0.63,0.69,-0.64,0.69,-0.51,0.78,-0.67,0.83,-0.8,0.76,-0.49,0.24,-0.14,0.1,-0.08,0.06,-0.04,0.03,-0.02,0.02,-0.02,0.02,-0.01,0.01,-0.02,0.01,-0.01,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,"
     ]
    }
   ],
   "source": [
    "test = get_pcm_data_from_wav_file_w_librosa('TR-1002-A', 'TR-1007-A_-hi-tech.wav', 44100)\n",
    "\n",
    "for ele in test:\n",
    "    print(str(ele)+',', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0,-0.59,0.27,-0.15,0.31,-0.34,0.12,-0.06,0.48,-0.29,0.19,-0.22,0.4,-0.2,0.1,-0.35,0.37,-0.22,0.16,-0.43,0.27,-0.13,0.23,-0.37,0.25,-0.19,0.47,-0.3,0.2,-0.12,0.48,-0.38,0.3,-0.37,0.53,-0.37,0.25,-0.55,0.47,-0.36,0.35,-0.58,0.47,-0.32,0.46,-0.63,0.5,-0.37,0.57,-0.4,0.42,-0.45,0.6,-0.57,0.52,-0.57,0.52,-0.64,0.68,-0.87,0.9,-0.66,0.75,-0.49,0.51,-0.38,0.45,-0.44,0.33,-0.28,0.4,-0.4,0.31,-0.4,0.44,-0.4,0.29,-0.36,0.45,-0.3,0.34,-0.44,0.48,-0.29,0.31,-0.39,0.21,-0.12,0.37,-0.41,0.38,-0.31,0.43,-0.45,0.35,-0.43,0.39,-0.4,0.33,-0.4,0.41,-0.33,0.35,-0.44,0.41,-0.41,0.33,-0.43,0.41,-0.38,0.43,-0.36,0.36,-0.36,0.41,-0.56,0.58,-0.78,0.76,-0.57,0.79,-0.64,0.5,-0.65,0.75,-0.73,0.91,-0.74,0.74,-0.8,0.69,-0.91,0.93,-0.68,0.68,-0.82,0.58,-0.55,0.53,-1.0,0.82,-0.71,0.81,-0.84,0.82,-0.87,0.79,-0.64,0.42,-0.63,0.44,-0.4,0.45,-0.66,0.71,-0.55,0.91,-0.57,0.65,-0.77,0.86,-0.71,0.43,-0.68,0.44,-0.36,0.44,-0.54,0.76,-0.63,0.56,-0.43,0.47,-0.56,0.74,-0.79,0.86,-0.44,0.89,-0.66,0.66,-0.42,0.86,-0.75,0.34,-0.67,0.63,-0.73,0.75,-0.9,0.73,-0.48,0.63,-0.85,0.8,-0.26,0.78,-0.76,0.56,-0.6,0.71,-0.6,0.58,-0.78,0.71,-0.74,0.62,-0.85,0.82,-0.5,0.7,-0.72,0.78,-0.47,0.82,-0.62,0.49,-0.48,0.68,-0.92,0.54,-0.85,0.72,-0.63,0.57,-0.74,0.81,-0.52,0.55,-0.47,0.47,-0.55,0.85,-0.72,0.62,-0.59,0.41,-0.36,0.37,-0.8,0.65,-0.45,0.52,-0.29,0.29,-0.26,0.48,-0.54,0.55,-0.53,0.37,-0.31,0.31,-0.41,0.7,-0.47,0.51,-0.42,0.29,-0.32,0.3,-0.64,0.49,-0.5,0.49,-0.41,0.34,-0.24,0.65,-0.58,0.43,-0.54,0.33,-0.34,0.29,-0.48,0.65,-0.59,0.53,-0.44,0.41,-0.45,0.43,-0.59,0.52,-0.54,0.48,-0.43,0.42,-0.33,0.69,-0.54,0.48,-0.48,0.36,-0.37,0.39,-0.63,0.69,-0.67,0.55,-0.51,0.44,-0.55,0.55,-0.68,0.89,-0.55,0.48,-0.4,0.5,-0.53,0.81,-0.67,0.56,-0.51,0.49,-0.48,0.52,-0.67,0.55,-0.59,0.52,-0.69,0.76,-0.58,0.72,-0.6,0.67,-0.52,0.5,-0.54,0.66,-0.73,0.69,-0.79,0.58,-0.53,0.57,-0.64,0.63,-0.86,0.7,-0.63,0.61,-0.54,0.7,-0.7,0.77,-0.76,0.68,-0.66,0.76,-0.79,0.82,-0.73,0.61,-0.65,0.59,-0.63,0.64,-0.62,0.7,-0.69,0.66,-0.53,0.77,-0.78,0.82,-0.79,0.79,-0.55,0.28,-0.14,0.08,-0.06,0.04,-0.03,0.02,-0.02,0.03,-0.02,0.02,-0.02,0.02,-0.02,0.01,-0.01,0.01,-0.01,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,"
     ]
    }
   ],
   "source": [
    "test = get_pcm_data_from_wav_file_test('TR-1002-A', 'TR-1007-A_-hi-tech.wav')\n",
    "\n",
    "for ele in test:\n",
    "    print(str(ele)+',', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavy = wavio.read('TR-1002-A_-medieval.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44100"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_csv = get_most_recent_file_in_dir('./pcms/')\n",
    "\n",
    "retry_previous_failures('./pcms/'+prev_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
